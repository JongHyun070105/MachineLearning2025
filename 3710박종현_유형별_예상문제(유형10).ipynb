{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JongHyun070105/MachineLearning2025/blob/main/3710%EB%B0%95%EC%A2%85%ED%98%84_%EC%9C%A0%ED%98%95%EB%B3%84_%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C(%EC%9C%A0%ED%98%9510).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 유형별 예상문제(유형10)\n",
        "# 데이터셋 분리 및 스케일링"
      ],
      "metadata": {
        "id": "WOQNxuHow6dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01.\n",
        "#\ttrain_test_split()을 사용하여 데이터를 80:20으로 나누세요.\n",
        "# - 입력(X): mpg 컬럼을 제외한 모든 컬럼\n",
        "# - 타겟(y): mpg 컬럼\n",
        "\n",
        "X = df.drop(columns = ['mpg'])\n",
        "y = df['mpg']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ytest_size = 0.2)"
      ],
      "metadata": {
        "id": "t9Yy6YAsxHof"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 02.\n",
        "# StandardScaler를 적용하여 데이터를 스케일링하세요.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)"
      ],
      "metadata": {
        "id": "9DLhtcmxxcEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 03.\n",
        "# MinMaxScaler를 적용하여 데이터를 0~1 범위로 스케일링하세요.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler2 = MinMaxScaler()\n",
        "\n",
        "df_scaled2 = scaler2.fit_transform(df)\n",
        "df_scaled2 = pd.DataFrame(df_scaled2, columns=df.columns)"
      ],
      "metadata": {
        "id": "iiCkidghxgY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04.\n",
        "# RobustScaler를 적용하여 이상치 영향을 줄인 스케일링을 수행하세요.\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "robustScaler = RobustScaler()\n",
        "x_train_s = robustScaler.fit_transform(X_train)\n",
        "x_test_s = robustScaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "ltVCJk5OxkyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 05.\n",
        "# train_test_split()을 사용하여 데이터를 70:30으로 나누고, StandardScaler를 적용하세요.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "df_scaled = standard_scaler.fit_transform(df)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)"
      ],
      "metadata": {
        "id": "JOY22gsExk9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}